{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SPEECH EMOTION RECOGNITION","metadata":{"id":"b277ee55-6f56-45fc-b635-1bf676fb19f4"}},{"cell_type":"markdown","source":"## Introduction\nThe purpose of this project is to train a neural network to recognize emotion in english human speech.\n\n**Speech Emotion Recognition (SER)** will be defined to be the act of teaching a neural network to recognize and identify emotions, from a given number of emotions, found in english auditory phrases. The motivation is to mimic how humans can intepret additional meaning in phrases which is not always reflected in the definition of the words in that sentence.\n\nWe will be comparing the performance of two models. The first model is a **LSTM (Long Short-Term Memory)** model and the second model is a **CNN (Convolutional Neural Network)** model.","metadata":{"id":"49e170a9-551b-402f-93fb-bc337a7440d0"}},{"cell_type":"markdown","source":"## Datasets\nThe main dataset that will be used throughtout this project is the dataset labelled **Speech Emotion Recognition (en)** compiled by **Dmitry Babko** which can be found [here](https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en) on **[kaggle](https://www.kaggle.com/)**.\nIt is a combination of four popular datasets which are labelled **CREMA-D** (Crowd-sourced Emotional Multimodal Actors Dataset), **RAVDESS** (Ryerson Audio-Visual Database of Emotional Speech and Song), **SAVEE** (Surrey Audio-Visual Expressed Emotion) and **TESS** (Toronto Emotional Speech Set).","metadata":{"id":"4cd9e0ca-402c-451c-af75-faca6273b69b"}},{"cell_type":"markdown","source":"## Exploratory Analysis of Data\nLet us retrieve the data from these datasets so that we can peform some preprocessing and feature extraction steps, before we input it to our models. Each dataset includes multiple auditory phrases which are spoken in the different emotions and are the .wav file format.\n\nIn the **CREMA-D** dataset each file in structure in the following way: `1001_DFA_ANG_XX.wav`.\n\nWhat we care about is the third bit, or in this case, `ANG`, which represents the emotion.\n\nIn this dataset there are 6 unique emotions which are as follows:\n1. `ANG` - anger\n2. `DIS` - disgust\n3. `FEA` - fear\n4. `HAP` - happiness\n5. `NEU` - neutral\n6. `SAD` - sadness\n\nWith this in mind let us now retrieve the **CREMA-D** dataset.","metadata":{"id":"fd670c6f-3ade-45b6-ba7f-6a400042a12e"}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ncrema_path = '../input/speech-emotion-recognition-en/Crema'\n\ncrema_to_emotion_dct = {\n    'ANG': 'anger',\n    'DIS': 'disgust',\n    'FEA': 'fear',\n    'HAP': 'happiness',\n    'NEU': 'neutral',\n    'SAD': 'sadness',\n}\n\ncrema_paths = []\n\nfor file in os.listdir(crema_path):\n    emotion = crema_to_emotion_dct[file.split('_')[2]]\n    crema_paths.append((emotion, crema_path+'/'+file))\n\ncrema_df = pd.DataFrame.from_dict(crema_paths)\ncrema_df.rename(columns={0:'emotion',1:'path'}, inplace=True)\ncrema_df.head()","metadata":{"id":"707e9f01","outputId":"af7fc163-c79f-483c-d825-536b68f8d4c1","execution":{"iopub.status.busy":"2023-08-05T19:56:58.699899Z","iopub.execute_input":"2023-08-05T19:56:58.700248Z","iopub.status.idle":"2023-08-05T19:56:58.878392Z","shell.execute_reply.started":"2023-08-05T19:56:58.700218Z","shell.execute_reply":"2023-08-05T19:56:58.877381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the **RAVDESS** dataset each file in structure in the following way: `02-01-06-01-02-01-12.wav`.\n\nWhat we care about is the third bit, or in this case, `06`, which represents the emotion.\n\nIn this dataset there are 8 emotions which are as follows:\n1. `01` - neutral\n2. `02` - calm\n3. `03` - happiness\n4. `04` - sadness\n5. `05` - anger\n6. `06` - fear\n7. `07` - disgust\n8. `08` - surprise\n\nHowever due to the similarities between calm and neutral, aswell as a widely accepted belief by psychologists, originally proposed by Paul Ekman, which suggests that \"*...the six basic emotions are anger, disgust, fear, happiness, sadness, and surprise*\", we will merge them to limit our emotions to the 6 aforementioned as well as no emotion, being neutral.\n\nWith this in mind let us now retrieve the **RAVDESS** dataset.","metadata":{"id":"Kfgpt3jM-fqv"}},{"cell_type":"code","source":"ravdess_path = '../input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24'\n\nravdess_to_emotion_dct = {\n    '01': 'neutral',\n    '02': 'neutral',\n    '03': 'happiness',\n    '04': 'sadness',\n    '05': 'anger',\n    '06': 'fear',\n    '07': 'disgust',\n    '08': 'surprise',\n}\n\nravdess_paths = []\n\nfor folder in os.listdir(ravdess_path):\n    for file in os.listdir(ravdess_path+'/'+folder):\n        emotion = ravdess_to_emotion_dct[file.split('-')[2]]\n        ravdess_paths.append((emotion, ravdess_path+'/'+folder+'/'+file))\n\nravdess_df = pd.DataFrame.from_dict(ravdess_paths)\nravdess_df.rename(columns={0:'emotion',1:'path'}, inplace=True)\nravdess_df.head()","metadata":{"id":"OSaUjMEM0Y83","outputId":"ea98c22e-de4f-41e0-d549-11ba42b7b5af","execution":{"iopub.status.busy":"2023-08-05T19:56:58.880613Z","iopub.execute_input":"2023-08-05T19:56:58.881284Z","iopub.status.idle":"2023-08-05T19:56:59.072514Z","shell.execute_reply.started":"2023-08-05T19:56:58.881251Z","shell.execute_reply":"2023-08-05T19:56:59.071517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the **SAVEE** dataset each file in structure in the following way: `DC_a01.wav`.\n\nWhat we care about is the letter before the number, or in this case, `a`, which represents the emotion.\n\nIn this dataset there are 7 unique emotions which are as follows:\n1. `a` - anger\n2. `d` - disgust\n3. `f` - fear\n4. `h` - happiness\n5. `n` - neutral\n6. `sa` - sadness\n7. `su` - surprise\n\nWith this in mind let us now retrieve the **SAVEE** dataset.","metadata":{"id":"A7TlemRRAPrX"}},{"cell_type":"code","source":"savee_path = '../input/speech-emotion-recognition-en/Savee'\n\nsavee_to_emotion_dct = {\n    'a': 'anger',\n    'd': 'disgust',\n    'f': 'fear',\n    'h': 'happiness',\n    'n': 'neutral',\n    'sa': 'sadness',\n    'su': 'surprise',\n}\n\nsavee_paths = []\n\nfor file in os.listdir(savee_path):\n    x = file.split('_')\n    y = x[1][0:2] if x[1][0] == 's' else x[1][0]\n    emotion = savee_to_emotion_dct[y]\n    savee_paths.append((emotion, savee_path+'/'+file))\n\nsavee_df = pd.DataFrame.from_dict(savee_paths)\nsavee_df.rename(columns={0:'emotion',1:'path'}, inplace=True)\nsavee_df.head()","metadata":{"id":"F592cW1B03dC","outputId":"44aa12ee-8c62-41f4-aaba-411ec7ebada9","execution":{"iopub.status.busy":"2023-08-05T19:56:59.074060Z","iopub.execute_input":"2023-08-05T19:56:59.074398Z","iopub.status.idle":"2023-08-05T19:56:59.139691Z","shell.execute_reply.started":"2023-08-05T19:56:59.074370Z","shell.execute_reply":"2023-08-05T19:56:59.138581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the **TESS** dataset each file in structure in the following way: `OAF_back_angry.wav`.\n\nWhat we care about is the third bit, or in this case, `angry`, which represents the emotion.\n\nIn this dataset there are 7 unique emotions which are as follows:\n1. `angry` - anger\n2. `disgust` - disgust\n3. `fear` - fear\n4. `happy` - happiness\n5. `neutral` - neutral\n6. `sad` - sadness\n7. `ps` - surprise\n\nWith this in mind let us now retrieve the **TESS** dataset.","metadata":{"id":"KcBUTor1B1wZ"}},{"cell_type":"code","source":"tess_path = '../input/speech-emotion-recognition-en/Tess'\n\ntess_to_emotion_dct = {\n    'angry': 'anger',\n    'disgust': 'disgust',\n    'fear': 'fear',\n    'happy': 'happiness',\n    'neutral': 'neutral',\n    'ps': 'surprise', # pleasant surprise\n    'sad': 'sadness',\n}\n\ntess_paths = []\n\nfor folder in os.listdir(tess_path):\n    for file in os.listdir(tess_path+'/'+folder):\n        emotion = tess_to_emotion_dct[file.split('.')[0].split('_')[2]]\n        tess_paths.append((emotion, tess_path+'/'+folder+'/'+file))\n\ntess_df = pd.DataFrame.from_dict(tess_paths)\ntess_df.rename(columns={0:'emotion',1:'path'}, inplace=True)\ntess_df.head()","metadata":{"id":"Z6D8ny021Ife","outputId":"c0da1e02-9736-4438-ca91-e4858b0135a1","execution":{"iopub.status.busy":"2023-08-05T19:56:59.142764Z","iopub.execute_input":"2023-08-05T19:56:59.143116Z","iopub.status.idle":"2023-08-05T19:56:59.589429Z","shell.execute_reply.started":"2023-08-05T19:56:59.143083Z","shell.execute_reply":"2023-08-05T19:56:59.588273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us join the dataframes into one collective dataframe.","metadata":{"id":"H_Yl5KVo1eET"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn\n\nprint(crema_df.shape)\nprint(ravdess_df.shape)\nprint(savee_df.shape)\nprint(tess_df.shape)\nprint()\n\ndf = pd.concat([crema_df, ravdess_df, savee_df, tess_df], axis=0)\n\nprint(df.shape)\nprint()\n\n# Modified from: https://www.kaggle.com/code/lkergalipatak/speech-emotion-recognition-with-cnn?scriptVersionId=99779872&cellId=19\ndef plot_emotion_count(df):\n    plt.figure(figsize=(12,6))\n    plt.title('Emotions count')\n    \n    emotions = seaborn.countplot(x='emotion', data=df, palette='Set3')\n    \n    plt.xlabel('Emotions')\n    plt.ylabel('Count')\n    \n    return plt\n\nplot_emotion_count(df).show()","metadata":{"id":"-pjO4TsQ1vig","outputId":"76a0610e-3c5f-4232-bbbb-2df587f0af7e","execution":{"iopub.status.busy":"2023-08-05T19:56:59.591356Z","iopub.execute_input":"2023-08-05T19:56:59.591745Z","iopub.status.idle":"2023-08-05T19:57:00.661481Z","shell.execute_reply.started":"2023-08-05T19:56:59.591710Z","shell.execute_reply":"2023-08-05T19:57:00.660545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So all together we have 12162 different audio files covering 7 unique emotions which we will train our models to recognize.","metadata":{"id":"pooI2I4TaURX"}},{"cell_type":"markdown","source":"### Preprocessing\nTo correctly preprocess the data we must first undertake the challenge of understanding how humans convey emotion within their speech. We will propose that this can be done by observing the characterisitics of audio or sound. \"*...the elements of sound should be listed separately as: pitch, duration, loudness, timbre, texture and spatial location*\" (Russell Burton, 2015).\n\nWe would like to focus on pitch and duration as the key features to augment in each file. The reasoning being that not everyone has the same pitch, humans either have higher or lower voices and talk either faster or slower, changing the duration of the audio.\n\nAnother way to augment the audio will be to add noise to the sound. The idea behind this is to perhaps cover some of the audio with this noise to help the model identify when the actor is talking better.\n\nThe functions to augment the data are as follows.\n\n","metadata":{"id":"6F18O80vE5kx"}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport librosa\n\ndef add_noise(data):\n    '''Signal to noise ratio is as follows: SNR = 2 * (A_signal - A_noise)\n    according to the derivation on https://en.wikipedia.org/wiki/Signal-to-noise_ratio.'''\n    A_signal = math.sqrt(np.mean(data**2))\n    noise = np.random.normal(0, A_signal/3, data.shape)\n    A_noise = math.sqrt(np.mean(noise**2))\n    SNR = 2 * (A_signal - A_noise)\n    return data + noise * SNR\n\ndef pitch_audio(data, sr, pitch_factor):\n    '''A positive value increases the pitch, a negative value decreases it.\n    For example, pitch_factor = 2.0 increases the pitch by 2 semitones (one octave).'''\n    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=pitch_factor)\n\ndef stretch_audio(data):\n    '''Stretch the data a certain amount.'''\n    amount = np.random.random()\n    amount = amount if amount > 0.45 else amount + 0.45 # want to make sure audio isn't too long\n    stretched_data = librosa.effects.time_stretch(y=data, rate=amount)\n    return librosa.util.fix_length(data=stretched_data, size=len(data))","metadata":{"id":"1b16d8cd","execution":{"iopub.status.busy":"2023-08-05T19:57:00.664137Z","iopub.execute_input":"2023-08-05T19:57:00.664877Z","iopub.status.idle":"2023-08-05T19:57:00.678656Z","shell.execute_reply.started":"2023-08-05T19:57:00.664848Z","shell.execute_reply":"2023-08-05T19:57:00.677683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us observe the effect this has on our data through analysing the waveform, spectrogram and listening to the audio itself.\n\nTo do this let us also create and import the following functions.","metadata":{"id":"N4h7e252aNG9"}},{"cell_type":"code","source":"from IPython.display import Audio\n\n# Modified from: https://www.kaggle.com/code/blitzapurv/speech-emotion-recognition-using-lstm?scriptVersionId=113306435&cellId=15\ndef generate_waveform(emotion, augmentation, filename, data, sr):\n    '''Create a waveform with the data provided.'''\n    plt.figure(figsize=(9, 4))\n    plt.title(f'{augmentation.title()} {emotion.lower()} waveform | {filename}')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude (% Scale)') # time - floating point\n    librosa.display.waveshow(y=data, sr=sr)\n    return plt\n\n# Modified from: https://www.kaggle.com/code/blitzapurv/speech-emotion-recognition-using-lstm?scriptVersionId=113306435&cellId=15\ndef generate_spectrogram(emotion, augmentation, filename, data, sr):\n    '''Create a spectrogram with the data provided.'''\n    x = librosa.stft(data) # time - frequency domain\n    xdB = librosa.amplitude_to_db(abs(x))\n    plt.figure(figsize=(9, 4))\n    plt.title(f'{augmentation.title()} {emotion.lower()} spectrogram | {filename}')\n    librosa.display.specshow(xdB, sr=sr, x_axis='time', y_axis='hz')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    cb = plt.colorbar()\n    cb.set_label('Amplitude (dB)', rotation=270, labelpad=20)\n    return plt","metadata":{"id":"tUSZEvnwdwFl","execution":{"iopub.status.busy":"2023-08-05T19:57:00.680045Z","iopub.execute_input":"2023-08-05T19:57:00.680415Z","iopub.status.idle":"2023-08-05T19:57:00.695692Z","shell.execute_reply.started":"2023-08-05T19:57:00.680380Z","shell.execute_reply":"2023-08-05T19:57:00.694671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us retrieve the audio we will be observing these changes on.","metadata":{"id":"xDallOF2eV6Y"}},{"cell_type":"code","source":"filename = df.iat[0,1]\nemotion = df.iat[0,0]","metadata":{"id":"8qxpc6Yre9KG","execution":{"iopub.status.busy":"2023-08-05T19:57:00.696841Z","iopub.execute_input":"2023-08-05T19:57:00.697650Z","iopub.status.idle":"2023-08-05T19:57:00.711710Z","shell.execute_reply.started":"2023-08-05T19:57:00.697601Z","shell.execute_reply":"2023-08-05T19:57:00.710686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now view and listen to the audio.","metadata":{"id":"oechSHMofhR7"}},{"cell_type":"code","source":"data, sr = librosa.load(filename)\n\ngenerate_waveform(emotion, 'original', filename, data, sr).show()\nprint()\ngenerate_spectrogram(emotion, 'original', filename, data, sr).show()\nprint()\n\nAudio(data=data, rate=sr)","metadata":{"id":"kc-6VL8zftPW","outputId":"45acaf61-216f-48f9-a432-6799090c8653","execution":{"iopub.status.busy":"2023-08-05T19:57:00.712919Z","iopub.execute_input":"2023-08-05T19:57:00.713506Z","iopub.status.idle":"2023-08-05T19:57:10.636581Z","shell.execute_reply.started":"2023-08-05T19:57:00.713473Z","shell.execute_reply":"2023-08-05T19:57:10.635666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us perform the noise augmentation and observe the changes.","metadata":{"id":"M2C7sYZ6gVhn"}},{"cell_type":"code","source":"noised_data = add_noise(data)\n\ngenerate_waveform(emotion, 'noised', filename, noised_data, sr).show()\nprint()\ngenerate_spectrogram(emotion, 'noised', filename, noised_data, sr).show()\nprint()\n\nAudio(data=noised_data, rate=sr)","metadata":{"id":"QMBz6GwDgbkd","outputId":"8f4ddabb-0c4e-43f9-e36b-af1d28c56198","execution":{"iopub.status.busy":"2023-08-05T19:57:10.641447Z","iopub.execute_input":"2023-08-05T19:57:10.642909Z","iopub.status.idle":"2023-08-05T19:57:11.734470Z","shell.execute_reply.started":"2023-08-05T19:57:10.642872Z","shell.execute_reply":"2023-08-05T19:57:11.733645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us peform the pitch augmentation and observe the changes.","metadata":{"id":"E9GVwnBdhGUT"}},{"cell_type":"code","source":"pitched_data = pitch_audio(data, sr, -3.0)\n\ngenerate_waveform(emotion, 'pitched', filename, pitched_data, sr).show()\nprint()\ngenerate_spectrogram(emotion, 'pitched', filename, pitched_data, sr).show()\nprint()\n\nAudio(data=pitched_data, rate=sr)","metadata":{"id":"cmO5_UnshOVV","outputId":"b412a21f-b700-4567-9f28-237ddf507a0e","execution":{"iopub.status.busy":"2023-08-05T19:57:11.735866Z","iopub.execute_input":"2023-08-05T19:57:11.736830Z","iopub.status.idle":"2023-08-05T19:57:13.882415Z","shell.execute_reply.started":"2023-08-05T19:57:11.736794Z","shell.execute_reply":"2023-08-05T19:57:13.881320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally let us perform the stretch augmentation and observe the changes.","metadata":{"id":"iwruW0kShrIn"}},{"cell_type":"code","source":"stretched_data = stretch_audio(data)\n\ngenerate_waveform(emotion, 'stretched', filename, stretched_data, sr).show()\nprint()\ngenerate_spectrogram(emotion, 'stretched', filename, stretched_data, sr).show()\nprint()\n\nAudio(data=stretched_data, rate=sr)","metadata":{"id":"hizVvpLshyL5","outputId":"1e98fd44-2e78-455c-ba25-f0d868d9ce36","execution":{"iopub.status.busy":"2023-08-05T19:57:13.883909Z","iopub.execute_input":"2023-08-05T19:57:13.884550Z","iopub.status.idle":"2023-08-05T19:57:14.925235Z","shell.execute_reply.started":"2023-08-05T19:57:13.884512Z","shell.execute_reply":"2023-08-05T19:57:14.924303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To aid the model with it's learning, it would also be beneficial to extract key features from the audio and input those features to our model.\n\nThe features we will be abstracting are the following:\n1. **MFCC (Mel Frequency Cepstral Coefficcients)** - Often used to describe timbre in MIR (Music Information Retrieval). As it is related to one of the characteristics of voice and sound, it will be useful when recognizing emotion.\n2. **RMS (Root Mean Square)** - Used to describe intesity or amplitude of a sound wave. Emotions can often alter the loudness of one's voice so it will be useful to analyze.\n3. **ZCR (Zero Crossing Rate)** - Detects the amount of times a sound wave changes between being positive and negative. Emotions can heighten or lessen this effect so it will be useful.\n4. **F0 (Fundamental Frequency)** - \"*perceived as pitch, [it] is the first and arguably most salient auditory component humans are exposed*\" to and it conveys emotion within human speech, so it will be useful (Liquan Liu, 2022).\n5. **Jitter** - Caused by irregular vocal cord vibration which disturbs the frequency of the sound, large irregularities can be correlated to emotion as the cause, which will make it useful for recognition.\n6. **Shimmer** - The same as jitter but it disturbs the amplitude of the sound, so it will be useful.\n7. **Speech Rate** - Emotions can cause one to talk faster or slower due to stress which will make this useful for identification.\n\nThe functions to extract this data are as follows.","metadata":{"id":"5nbz-plgib6I"}},{"cell_type":"code","source":"!pip install praat-parselmouth","metadata":{"execution":{"iopub.status.busy":"2023-08-05T19:57:14.926540Z","iopub.execute_input":"2023-08-05T19:57:14.927473Z","iopub.status.idle":"2023-08-05T19:57:28.080250Z","shell.execute_reply.started":"2023-08-05T19:57:14.927435Z","shell.execute_reply":"2023-08-05T19:57:28.078978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import parselmouth\n\ndef extract_mfcc(data, sr,flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)\n\ndef extract_rms(data):\n    return np.squeeze(librosa.feature.rms(y=data))\n\ndef extract_zcr(data):\n    return np.squeeze(librosa.feature.zero_crossing_rate(data))\n\ndef extract_F0(data,sr):\n    '''Extract F0 fundamental frequency from data.'''\n    pitches, magnitudes = librosa.piptrack(y=data, sr=sr)\n    return pitches[np.argmax(magnitudes, axis=0), np.arange(magnitudes.shape[1])]\n\n# Modified from: https://programtalk.com/python-more-examples/parselmouth.praat.call/\ndef extract_jitter(filename):\n    '''Extract frequency jitter from data.'''\n    sound = parselmouth.Sound(filename)\n    pointProcess = parselmouth.praat.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n    return parselmouth.praat.call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n\n# Modified from: https://programtalk.com/python-more-examples/parselmouth.praat.call/\ndef extract_shimmer(filename):\n    '''Extract amplitude shimmer from data.'''\n    sound = parselmouth.Sound(filename)\n    pointProcess = parselmouth.praat.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n    return parselmouth.praat.call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n\ndef extract_speech_rate(data):\n    return np.sum(librosa.zero_crossings(data)) / len(data)","metadata":{"id":"SeH6RBHvqEZF","execution":{"iopub.status.busy":"2023-08-05T19:57:28.085098Z","iopub.execute_input":"2023-08-05T19:57:28.087821Z","iopub.status.idle":"2023-08-05T19:57:28.169081Z","shell.execute_reply.started":"2023-08-05T19:57:28.087781Z","shell.execute_reply":"2023-08-05T19:57:28.168125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have completed our data augmentation and feature extraction functions, we can now complete the preprocessing step by applying these function to each file in the collective dataframe.\n\nHowever for each of the audios we can obtain a total of 12 different audios by combining all, some or none of our augmentations together.\n\nThey are as follows:\n1. Original\n2. Noised\n3. Pitched higher\n4. Pitched lower\n5. Strecthed\n6. Noised and pitched higher\n7. Noised and pitched lower\n8. Noised and stretched\n9. Pitched higher and stretched\n10. Pitched lower and stretched\n11. Noised, pitched higher and stretched\n12. Noised, pitched lower and stretched\n\nFirst let us create a function to extract all of the features at once.","metadata":{"id":"UCDglYwpssN0"}},{"cell_type":"code","source":"# Modified from: https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d?scriptVersionId=57350093&cellId=48\ndef extract_features(data, sr, path):\n    return np.hstack((np.array([]),\n      extract_zcr(data),\n      extract_rms(data),\n      extract_mfcc(data, sr),\n      extract_F0(data, sr),\n      extract_jitter(path),\n      extract_shimmer(path),\n      extract_speech_rate(data)\n      ))","metadata":{"id":"hF8b9lUtvDoW","execution":{"iopub.status.busy":"2023-08-05T19:57:28.174312Z","iopub.execute_input":"2023-08-05T19:57:28.176548Z","iopub.status.idle":"2023-08-05T19:57:28.184377Z","shell.execute_reply.started":"2023-08-05T19:57:28.176514Z","shell.execute_reply":"2023-08-05T19:57:28.183401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us extract all these feature from each of our augmented audios, including the original audio.","metadata":{"id":"1KT_dg-CwkwG"}},{"cell_type":"code","source":"# Modified from: https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d?scriptVersionId=57350093&cellId=50\ndef augment_data(df):\n    '''Augments the data according to the afore defined functions, then extracts the necessary features from it.'''\n    print(\"Preprocessing data\")\n    for path, emotion, i in zip(df.path, df.emotion, range(df.path.shape[0])):\n\n        X, Y = [], []\n        \n        # original data\n        duration=2.5\n        offset=0.6\n        data, sr = librosa.load(path, duration=duration, offset=offset)\n\n        lst = [data]\n\n        # 1 augmentation\n        lst.append(add_noise(data)) # noised\n        lst.append(pitch_audio(data, sr, pitch_factor=2.0)) # pitched higher\n        lst.append(pitch_audio(data, sr, pitch_factor=-2.0)) # pitched lower\n        lst.append(stretch_audio(data)) # stretched\n\n        # 2 augmentations\n        lst.append(add_noise(lst[2])) # noised pitched higher\n        lst.append(add_noise(lst[3])) # noised pitched lower\n        lst.append(add_noise(lst[4])) # noised stretched\n\n        lst.append(stretch_audio(lst[2])) # pitched higher stretched\n        lst.append(stretch_audio(lst[3])) # pitched lower stretched\n\n        # 3 augmentations\n        lst.append(add_noise(lst[8])) # noised pitched higher stretched\n        lst.append(add_noise(lst[9])) # noised pitched lower stretched\n\n        # process features\n        for element in lst:\n            X.append(extract_features(element, sr, path))\n            Y.append(emotion)\n            \n        yield X, Y\n\n        if i % 100 == 0:\n            print(f\"{i} files have been processed\")\n            \n    print(\"Done\")","metadata":{"id":"KdJHFr8G6uSH","execution":{"iopub.status.busy":"2023-08-05T19:57:28.189137Z","iopub.execute_input":"2023-08-05T19:57:28.191870Z","iopub.status.idle":"2023-08-05T19:57:28.207796Z","shell.execute_reply.started":"2023-08-05T19:57:28.191836Z","shell.execute_reply":"2023-08-05T19:57:28.206746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df = pd.DataFrame()\nY = []\n\nfor x, y in augment_data(df):\n    processed_df = pd.concat([processed_df, pd.DataFrame(x)])\n    for emo in y:\n        Y.append(emo)\n\nprocessed_df['emotion'] = Y\nprocessed_df = processed_df.fillna(0)\n\n# save features\nfeatures_path = \"./features.csv\"\nprocessed_df.to_csv(features_path, index=False)","metadata":{"id":"JzFqpzccoS_v","outputId":"0c0f9610-bee4-453b-cd7b-f3581c4436f1","execution":{"iopub.status.busy":"2023-08-05T19:57:28.212581Z","iopub.execute_input":"2023-08-05T19:57:28.215346Z","iopub.status.idle":"2023-08-05T19:59:53.862142Z","shell.execute_reply.started":"2023-08-05T19:57:28.215311Z","shell.execute_reply":"2023-08-05T19:59:53.860990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df.shape","metadata":{"id":"fpN3eND1PTgq","outputId":"d1ac2ad2-1536-456d-98b9-d564e1b4ced5","execution":{"iopub.status.busy":"2023-08-05T19:59:53.863496Z","iopub.execute_input":"2023-08-05T19:59:53.864226Z","iopub.status.idle":"2023-08-05T19:59:53.872421Z","shell.execute_reply.started":"2023-08-05T19:59:53.864188Z","shell.execute_reply":"2023-08-05T19:59:53.871419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df.head()","metadata":{"id":"isq9MdLVq0Hs","outputId":"7408cd99-7c9b-4d4d-a4c4-a27b4c752588","execution":{"iopub.status.busy":"2023-08-05T19:59:53.873846Z","iopub.execute_input":"2023-08-05T19:59:53.874205Z","iopub.status.idle":"2023-08-05T19:59:53.917912Z","shell.execute_reply.started":"2023-08-05T19:59:53.874172Z","shell.execute_reply":"2023-08-05T19:59:53.916740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The preprocessing is now complete.\n\nWe have a total of 7 emotion classes to train the models on, which are:\n1. anger\n2. disgust\n3. fear\n4. happiness\n5. neutral\n6. sadness\n7. surprise\n\nNow let us input the extracted features into the models.\n","metadata":{"id":"p7CcgaS7oWEe"}},{"cell_type":"markdown","source":"## Models\nWe will be conducting a comparison between two models to decide which model performs this recognition task with greater accuracy.\n\nThe first model will be the LSTM (Long Short-Term Memory) model and the second model will be the CNN (Convolutional Neural Network) model.\n\nThe LSTM model was chosen as the first model since it \"*...is able to learn long range dependencies using a combination of forget, input and output gates*\" (Hochreiter & Schmidhuber, 1998). Thus for a time based recognition problem like this one, it can be applied.\n\nThe CNN Model was chosen as the second model since it \"*...uses the already supplied dataset to it for training purposes, and predicts the possible future labels to be assigned*\" and \"*it processes all the layers, and hence detects all the underlying features, automatically*\" (SSLA, 2013). Since our datasets are labelled, this is a prediction/recognition task and we have preprocessed a bunch of the audio's features, it can be applied.\n","metadata":{"id":"a615f32a-0fe1-4a25-812d-0f3a6203e3f4"}},{"cell_type":"markdown","source":"First let us begin with transforming the preprocessed data, such that it can be provided to the models.\n\nTo do this let us first split our data into training and testing segments. We will use a standard split where 80% of our data will be used for testing and 20% will be used for evaluation. We will also shuffle the order of the data to avoid getting stuck in a local minimum. The `X` will be the extracted features from every piece of data and `Y` will be the emotion label for those pieces of data.","metadata":{"id":"S9LY81dgEcUI"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = processed_df.iloc[:, : -1]\nY = processed_df.iloc[:, -1]\n\nlb = LabelEncoder()\nY = lb.fit_transform(Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, test_size=0.2, shuffle=True)","metadata":{"id":"LPv2ru8CulCE","execution":{"iopub.status.busy":"2023-08-05T19:59:53.919381Z","iopub.execute_input":"2023-08-05T19:59:53.919879Z","iopub.status.idle":"2023-08-05T19:59:53.952779Z","shell.execute_reply.started":"2023-08-05T19:59:53.919840Z","shell.execute_reply":"2023-08-05T19:59:53.951574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us also encode the `Y` labels to be an interger which is mapped to one of the seven emotion classes.","metadata":{"id":"L9bzIXzBIz7g"}},{"cell_type":"code","source":"from keras.utils import to_categorical\n\nY_train_one_hot = to_categorical(Y_train, num_classes=7)\nY_test_one_hot = to_categorical(Y_test, num_classes=7)","metadata":{"id":"Bab2oJyaI1Ro","execution":{"iopub.status.busy":"2023-08-05T19:59:53.954933Z","iopub.execute_input":"2023-08-05T19:59:53.955813Z","iopub.status.idle":"2023-08-05T20:00:01.612072Z","shell.execute_reply.started":"2023-08-05T19:59:53.955773Z","shell.execute_reply":"2023-08-05T20:00:01.611092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us standardize our `X` data like so, to avoid errors within the models.","metadata":{"id":"lGyLVA1WJJh7"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"id":"nfNdMRyXJh3d","execution":{"iopub.status.busy":"2023-08-05T20:00:01.613552Z","iopub.execute_input":"2023-08-05T20:00:01.614387Z","iopub.status.idle":"2023-08-05T20:00:01.738882Z","shell.execute_reply.started":"2023-08-05T20:00:01.614351Z","shell.execute_reply":"2023-08-05T20:00:01.737922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally before we use the models let us define some callback functions which we can use to monitor the models.\n\nAn early stopping function can be used to stop training once we are happy with the results we are producing.\n","metadata":{"id":"6A_jOM-dnuFI"}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_acc', mode='auto', patience=5, restore_best_weights=True)","metadata":{"id":"Ec8NKh3aruTE","execution":{"iopub.status.busy":"2023-08-05T20:00:01.740269Z","iopub.execute_input":"2023-08-05T20:00:01.740744Z","iopub.status.idle":"2023-08-05T20:00:01.746852Z","shell.execute_reply.started":"2023-08-05T20:00:01.740708Z","shell.execute_reply":"2023-08-05T20:00:01.745683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A reduce learning rate function can be used to help the function learn quicker at certain times.","metadata":{"id":"mzcJoCs4rfTo"}},{"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)","metadata":{"id":"zxVwI_jvr1bc","execution":{"iopub.status.busy":"2023-08-05T20:00:01.748375Z","iopub.execute_input":"2023-08-05T20:00:01.749268Z","iopub.status.idle":"2023-08-05T20:00:01.758646Z","shell.execute_reply.started":"2023-08-05T20:00:01.749230Z","shell.execute_reply":"2023-08-05T20:00:01.757547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also define the batch size and number of epochs for both models.","metadata":{"id":"oq3iNQrmwvnv"}},{"cell_type":"code","source":"epochs = 20\nbatch_size = 128","metadata":{"id":"-QkQgDYOw1Rf","execution":{"iopub.status.busy":"2023-08-05T20:00:01.762350Z","iopub.execute_input":"2023-08-05T20:00:01.762661Z","iopub.status.idle":"2023-08-05T20:00:01.770435Z","shell.execute_reply.started":"2023-08-05T20:00:01.762610Z","shell.execute_reply":"2023-08-05T20:00:01.769380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model\n\nLet us now define our LSTM model as the following.\n\n* The LSTM layers will be the layers which are mainly tasked with processing the data.\n* The Dropout layers will prevent overfitting.\n* The Flatten layer will turn the data into something the Dense layer can process.\n* The BatchNormalization layer will be used to standardize our data further.\n* And finally the Dense layers will be tasked with producing the final classification.\n\nWe will use the **rmsprop** optimizer due to it's fast convergence on large models. We will also use the **categorical cross-entropy** loss function also known as **softmax**, because of it's use for multiclass categorization and because we have one hot encoded our labels.\n\nHere is the model below.","metadata":{"id":"IbtUnNH0saBg"}},{"cell_type":"code","source":"from keras import models, layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, BatchNormalization, Flatten\nfrom keras.metrics import Recall, Precision\n\ndef create_LSTM():\n    LSTM_model = Sequential()\n\n    LSTM_model.add(LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n    LSTM_model.add(Dropout(0.2))\n\n    LSTM_model.add(LSTM(64, return_sequences=True))\n    LSTM_model.add(Dropout(0.2))\n\n    LSTM_model.add(Flatten())\n    LSTM_model.add(Dense(64, activation='relu'))\n\n    LSTM_model.add(BatchNormalization())\n    LSTM_model.add(Dense(7, activation='softmax'))\n\n    LSTM_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc', Precision(), Recall()])\n\n    LSTM_model.summary()\n    \n    return LSTM_model","metadata":{"id":"C6yQx0OMsiF3","outputId":"747be0fa-9008-4f82-dd19-449320959e13","execution":{"iopub.status.busy":"2023-08-05T20:00:01.772061Z","iopub.execute_input":"2023-08-05T20:00:01.772830Z","iopub.status.idle":"2023-08-05T20:00:01.784854Z","shell.execute_reply.started":"2023-08-05T20:00:01.772792Z","shell.execute_reply":"2023-08-05T20:00:01.783861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now run and save our LSTM model.","metadata":{"id":"0MTdFNKuhWpc"}},{"cell_type":"code","source":"def run_LSTM(LSTM_model):\n    LSTM_output = LSTM_model.fit(X_train, Y_train_one_hot, validation_data=(X_test, Y_test_one_hot),\n                        epochs=epochs, batch_size=batch_size,\n                        callbacks=[early_stopping, reduce_lr])\n    \n    LSTM_json_path = './LSTM_model.json'\n    LSTM_h5_path = './LSTM_model.h5'\n    \n    with open(LSTM_json_path, 'w') as f:\n        f.write(LSTM_model.to_json())\n    LSTM_model.save_weights(LSTM_h5_path)\n\n    LSTM_eval = LSTM_model.evaluate(X_test, Y_test_one_hot)\n    LSTM_Y_pred = LSTM_model.predict(X_test)\n    \n    return LSTM_output, LSTM_eval, LSTM_Y_pred","metadata":{"id":"Fgglq1oRhcIJ","execution":{"iopub.status.busy":"2023-08-05T20:00:01.788511Z","iopub.execute_input":"2023-08-05T20:00:01.788904Z","iopub.status.idle":"2023-08-05T20:00:01.799052Z","shell.execute_reply.started":"2023-08-05T20:00:01.788873Z","shell.execute_reply":"2023-08-05T20:00:01.797918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LSTM_output, LSTM_eval, LSTM_Y_pred = run_LSTM(create_LSTM())","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:00:01.805720Z","iopub.execute_input":"2023-08-05T20:00:01.806660Z","iopub.status.idle":"2023-08-05T20:00:59.992224Z","shell.execute_reply.started":"2023-08-05T20:00:01.806603Z","shell.execute_reply":"2023-08-05T20:00:59.991065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Model\n\nLet us now define our CNN model as the following.\n\n* The Conv1D layers will be the layers which are mainly tasked with processing the data.\n* The BatchNormalization layers will be used to standardize our data further.\n* The MaxPooling1D layers will reduce the size of our data.\n* The Flatten layer will turn the data into something the Dense layer can process.\n* And finally the Dense layers will be tasked with producing the final classification.\n\nWe will use the **rmsprop** optimizer due to it's fast convergence on large models. We will also use the **categorical cross-entropy** loss function also known as **softmax**, because of it's use for multiclass categorization and because we have one hot encoded our labels.\n\nHere is the model below.","metadata":{"id":"hH3661lciugx"}},{"cell_type":"code","source":"from keras.layers import Conv1D, MaxPooling1D\n\n# Modified from: https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d?scriptVersionId=57350093&cellId=68\ndef create_CNN():\n    CNN_model = Sequential()\n\n    CNN_model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1], 1)))\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n\n    CNN_model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n\n    CNN_model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n\n    CNN_model.add(Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n\n    CNN_model.add(Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n\n    CNN_model.add(Flatten())\n    CNN_model.add(Dense(512, activation='relu'))\n\n    CNN_model.add(BatchNormalization())\n    CNN_model.add(Dense(7, activation='softmax'))\n\n    CNN_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc', Precision(), Recall()])\n\n    CNN_model.summary()\n    \n    return CNN_model","metadata":{"id":"Z1P8XR3OkpyH","outputId":"38b61773-8dd6-41d3-b141-300688e485ce","execution":{"iopub.status.busy":"2023-08-05T20:00:59.994083Z","iopub.execute_input":"2023-08-05T20:00:59.994460Z","iopub.status.idle":"2023-08-05T20:01:00.006819Z","shell.execute_reply.started":"2023-08-05T20:00:59.994427Z","shell.execute_reply":"2023-08-05T20:01:00.005742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now run and save our CNN model.","metadata":{"id":"fiq58Dlni1mJ"}},{"cell_type":"code","source":"def run_CNN(CNN_model):\n    CNN_output = CNN_model.fit(X_train, Y_train_one_hot, validation_data=(X_test, Y_test_one_hot),\n                        epochs=epochs, batch_size=batch_size,\n                        callbacks=[early_stopping, reduce_lr])\n\n    CNN_json_path = './CNN_model.json'\n    CNN_h5_path = './CNN_model.h5'\n    \n    with open(CNN_json_path, 'w') as f:\n        f.write(CNN_model.to_json())\n    CNN_model.save_weights(CNN_h5_path)\n\n    CNN_eval = CNN_model.evaluate(X_test, Y_test_one_hot)\n    CNN_Y_pred = CNN_model.predict(X_test)\n    \n    return CNN_output, CNN_eval, CNN_Y_pred","metadata":{"id":"lB9jKig5i0IV","execution":{"iopub.status.busy":"2023-08-05T20:01:00.008285Z","iopub.execute_input":"2023-08-05T20:01:00.008706Z","iopub.status.idle":"2023-08-05T20:01:00.024006Z","shell.execute_reply.started":"2023-08-05T20:01:00.008674Z","shell.execute_reply":"2023-08-05T20:01:00.023028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN_output, CNN_eval, CNN_Y_pred = run_CNN(create_CNN())","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:01:00.025662Z","iopub.execute_input":"2023-08-05T20:01:00.026082Z","iopub.status.idle":"2023-08-05T20:01:45.284562Z","shell.execute_reply.started":"2023-08-05T20:01:00.026046Z","shell.execute_reply":"2023-08-05T20:01:45.283426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have run both of our models, let's examine and compare the results.","metadata":{"id":"BgbGYHs-i7sp"}},{"cell_type":"markdown","source":"## Results\nTo be able to understand the results fully we must first reason why the metrics we will be using are appropriate.\n\n1. **Accuracy** - It is quite natural and intuitve to use this for this recognition task, since our dataset is finite and labelled. Thus accuracy can appropriately be used as a metric for our models.\n2. **Recall** - As it aims to identify the proportion of true positives in the relevant emotion and since our dataset is finite and labelled, like accuracy it can also be appropriatley used as a metric.\n3. **Precision** - As it aims to identify the proportion of correctly classified emotions within the models choise and since our dataset is finite and labelled, it is an appropriate metric.\n4. **F1 Score** - It is a combination of the recall and precision metrics and it identifies what percentage of classifications by the model was correct. Since our dataset is finite and labelled, it is an appropriate metric.\n\nWith this in mind let us now observe the results.","metadata":{"id":"acd8259d-b37c-4bfc-b8d0-ba3a1b1ca35e"}},{"cell_type":"markdown","source":"First let us compare the accuracies of the two models to get a general sense of which model can recognize the emotions better.","metadata":{}},{"cell_type":"code","source":"print(f'The accuracy of the LSTM model is: {round(LSTM_eval[1]*100, 2)}%')\nprint(f'The accuracy of the CNN model is: {round(CNN_eval[1]*100, 2)}%')\n\n# Modified from: \ndef plot_accuracy(model, output):\n    '''Plots the accuracy of the model compared to its epoch.'''\n    \n    train_acc = output.history['acc']\n    test_acc = output.history['val_acc']\n    \n    ticks = [i for i in range(len(train_acc))]\n\n    plt.plot(ticks, train_acc, label='Training accuracy')\n    plt.plot(ticks, test_acc, label='Testing accuracy')\n    plt.title(f'{model} Model accuracy')\n    plt.legend()\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    \n    return plt\n\nplot_accuracy('LSTM', LSTM_output).show()\nplot_accuracy('CNN', CNN_output).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:01:45.286489Z","iopub.execute_input":"2023-08-05T20:01:45.286894Z","iopub.status.idle":"2023-08-05T20:01:45.943528Z","shell.execute_reply.started":"2023-08-05T20:01:45.286858Z","shell.execute_reply":"2023-08-05T20:01:45.941050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus after 20 epochs we can see that the **CNN/LSTM** model had the higher accuracy.","metadata":{}},{"cell_type":"markdown","source":"Let us also vizualise the confusion matrix, along with the recall, precision and f1 score. The predicted values are the model's classification of the results and the actual values are the datasets classification of the results.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# Modified from: https://www.kaggle.com/code/ahmedabdelmon3m/speech-emotion-recognition?scriptVersionId=96230704&cellId=12\ndef print_stats(model, Y_test, Y_pred, encoder):\n    '''Prints the metrics along with returning the confusion matrix plot.'''\n    print(f'\\n{model} Model\\n{\"-\"*(len(model)+6)}')\n    print(classification_report(Y_test , Y_pred, target_names=encoder.classes_))\n\n    cm = confusion_matrix(Y_test, Y_pred, labels=list(range(0, 7)))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=encoder.classes_)\n    disp.plot()\n    \n    plt.title(f'{model} Confusion Matrix')\n    plt.xlabel('Predicted Values')\n    plt.xticks(rotation = 30)\n    plt.ylabel('Actual Values')\n    \n    return plt\n\n# Modified from: https://www.kaggle.com/code/ahmedabdelmon3m/speech-emotion-recognition?scriptVersionId=96230704&cellId=12\ndef mapping(arr):\n    '''Maps arr to the different emotion classes.'''\n    pred = []\n    for i in range(len(arr)):\n        idx = 0\n        max = 0\n        for j in range(7):\n            if arr[i][j] > max:\n                max = arr[i][j]\n                idx = j\n        pred.append(idx)\n    return pred","metadata":{"id":"_7ivdXt-SLrJ","execution":{"iopub.status.busy":"2023-08-05T20:01:45.944935Z","iopub.execute_input":"2023-08-05T20:01:45.945370Z","iopub.status.idle":"2023-08-05T20:01:45.957316Z","shell.execute_reply.started":"2023-08-05T20:01:45.945335Z","shell.execute_reply":"2023-08-05T20:01:45.956332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_stats('LSTM', list(Y_test), mapping(LSTM_Y_pred), lb).show()\nprint_stats('CNN', list(Y_test), mapping(CNN_Y_pred), lb).show()","metadata":{"id":"S4VcE2bvoY0Z","execution":{"iopub.status.busy":"2023-08-05T20:01:45.958542Z","iopub.execute_input":"2023-08-05T20:01:45.959005Z","iopub.status.idle":"2023-08-05T20:01:48.321295Z","shell.execute_reply.started":"2023-08-05T20:01:45.958972Z","shell.execute_reply":"2023-08-05T20:01:48.319548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the A model has higher results in the a, b, c metrics, where as the B model only has higher results in the d metric, from these results it should be conclusive to say the model A is better. But is that truly the case?\n","metadata":{}},{"cell_type":"markdown","source":"## Discussion\n`Discuss the results and analysis, provide some insight about system performance, including strengths, weaknesses, limitations and possible future work.`\n\nThe A model might have better results but is it also quicker and more space efficient than the B model. <!-- is it smaller than model B -->","metadata":{"id":"a1c8a3c5-3be1-4421-901e-7c9b7aeed382"}},{"cell_type":"markdown","source":"## Acknowledgements\nThanks to [this](https://www.kaggle.com/code/dmitrybabko/speech-emotion-recognition-conv1d) notebook which helped start us write our own notebook and verbalize our thoughts. **PROBABLY REMOVE THISE, ALSO MAKE SURE TO MAKE THE ENTIRE NOTEBOOK GENERIC, NO MY, WE, US, I RUBBISH**","metadata":{"id":"ba73e890-6311-4cc6-a5f5-4b2fc52bcfe8"}}]}